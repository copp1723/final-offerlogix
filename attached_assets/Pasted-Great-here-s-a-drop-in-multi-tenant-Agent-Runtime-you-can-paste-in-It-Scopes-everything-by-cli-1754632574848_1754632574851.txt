Great—here’s a drop-in, multi-tenant Agent Runtime you can paste in. It:
	•	Scopes everything by clientId (LLM prompts, Supermemory reads/writes)
	•	Pulls the active agent config per client (with safe fallbacks)
	•	Builds a clean system prompt (personality, tone, do/don’t lists)
	•	Recalls scoped memory for the conversation/lead
	•	Generates a reply and optional quick replies
	•	Logs/audits the exchange
	•	Stays boring/reliable (no clever magic)

I kept the code aligned with what you’ve already built: LLMClient and your Supermemory helpers. Adjust import paths to match your repo.

⸻

1) Schema migration (per-client agent configs)

Add client_id to ai_agent_config and enforce one active config per client.

-- migrations/20250808_add_client_to_ai_agent_config.sql
ALTER TABLE ai_agent_config ADD COLUMN client_id uuid REFERENCES clients(id);

-- Optional: default active = false if not present already
ALTER TABLE ai_agent_config ALTER COLUMN is_active SET DEFAULT false;

-- One active config per client
CREATE UNIQUE INDEX ai_agent_config_one_active_per_client
  ON ai_agent_config (client_id)
  WHERE is_active = true;

CREATE INDEX ai_agent_config_client_id_idx ON ai_agent_config (client_id);

Update your Drizzle schema:

// server/db/schema.ts (add clientId to aiAgentConfig)
export const aiAgentConfig = pgTable("ai_agent_config", {
  id: varchar("id").primaryKey().default(sql`gen_random_uuid()`),
  name: varchar("name").notNull(),
  tonality: text("tonality").notNull().default("professional"),
  personality: text("personality"),
  dosList: jsonb("dos_list").default([]).notNull(),
  dontsList: jsonb("donts_list").default([]).notNull(),
  industry: varchar("industry").default("automotive"),
  responseStyle: text("response_style").default("helpful"),
  isActive: boolean("is_active").default(false).notNull(),
  clientId: uuid("client_id").references(() => clients.id),  // ← new
  createdAt: timestamp("created_at").defaultNow().notNull(),
  updatedAt: timestamp("updated_at").defaultNow().notNull(),
});

Seed at least one config per client with isActive = true.

⸻

2) Drop-in runtime

Create server/services/agent-runtime.ts.

// server/services/agent-runtime.ts
import { storage } from "../storage";
import { aiAgentConfig as aiAgentConfigTable, leads as leadsTable } from "../db/schema";
import { eq, and } from "drizzle-orm";
import { LLMClient } from "../services/llm/LLMClient"; // your unified LLM client
import { supermemory } from "../services/memory/supermemory-client"; // your thin client wrapper
import crypto from "crypto";

type ModelId = "openai/gpt-4o-mini" | "openai/gpt-4o" | "anthropic/claude-3-5-sonnet" | string;

export interface AgentRuntimeReplyInput {
  clientId: string;
  message: string;
  leadId?: string;
  conversationId?: string;
  topic?: string; // optional hint (e.g. "pricing", "test drive", "service")
  model?: ModelId;
  maxTokens?: number;
}

export interface AgentRuntimeReply {
  reply: string;
  quickReplies?: string[];
  usedConfigId: string | null;
  memoryContextDocs?: Array<{ id: string; score: number; snippet: string }>;
}

interface ActiveAgentConfig {
  id: string;
  name: string;
  personality?: string;
  tonality: string;
  responseStyle: string;
  dosList: string[];
  dontsList: string[];
  industry?: string;
  model?: ModelId;
}

export class AgentRuntime {
  /**
   * Hash emails for memory partitioning without leaking PII.
   */
  private static hashEmail(email?: string | null) {
    if (!email) return "unknown";
    return crypto.createHash("sha256").update(email.trim().toLowerCase()).digest("hex");
  }

  /**
   * Load the active config for a client.
   */
  static async getActiveConfig(clientId: string): Promise<ActiveAgentConfig | null> {
    const rows = await storage.db
      .select()
      .from(aiAgentConfigTable)
      .where(and(eq(aiAgentConfigTable.clientId, clientId), eq(aiAgentConfigTable.isActive, true)))
      .limit(1);

    if (!rows?.[0]) return null;

    const r = rows[0] as any;
    return {
      id: r.id,
      name: r.name,
      personality: r.personality ?? undefined,
      tonality: r.tonality ?? "professional",
      responseStyle: r.responseStyle ?? "helpful",
      dosList: Array.isArray(r.dosList) ? r.dosList : [],
      dontsList: Array.isArray(r.dontsList) ? r.dontsList : [],
      industry: r.industry ?? "automotive",
      model: r.model || "openai/gpt-4o-mini",
    };
    // If you stored model in the same table add a column; otherwise omit.
  }

  /**
   * Compose the system prompt from config with safe, minimal rules.
   */
  static buildSystemPrompt(cfg: ActiveAgentConfig): string {
    const dos = cfg.dosList?.length ? `\nDo:\n- ${cfg.dosList.join("\n- ")}` : "";
    const donts = cfg.dontsList?.length ? `\nDon't:\n- ${cfg.dontsList.join("\n- ")}` : "";

    return [
      `You are an AI assistant for an automotive dealership platform.`,
      `Personality: ${cfg.personality || "professional"}.`,
      `Tonality: ${cfg.tonality}.`,
      `Response style: ${cfg.responseStyle}.`,
      `Goals: be concise, accurate, and helpful. Avoid over-promising or making up facts.`,
      `If scheduling or pricing is requested, propose next concrete step (book test drive, check inventory, connect sales).`,
      `Never claim human identity. Stay within dealership context.`,
      dos,
      donts,
    ].join("\n");
  }

  /**
   * Retrieve scoped memory for this conversation/lead.
   */
  static async recallMemory(opts: {
    clientId: string;
    leadId?: string;
    topic?: string;
  }) {
    const { clientId, leadId, topic } = opts;

    // pull lead for hashed email tag
    let lead: { email?: string | null } | null = null;
    if (leadId) {
      const rows = await storage.db.select().from(leadsTable).where(eq(leadsTable.id, leadId)).limit(1);
      lead = rows?.[0] || null;
    }

    const tags = [
      `client:${clientId}`,
      lead?.email ? `lead:${AgentRuntime.hashEmail(lead.email)}` : null,
    ].filter(Boolean) as string[];

    // conservative search parameters
    const query = topic && topic.trim() ? topic : "recent conversation context and similar successful replies";
    try {
      const res = await supermemory.search({
        q: query,
        limit: 5,
        documentThreshold: 0.6,
        onlyMatchingChunks: true,
        containerTags: tags,
      });

      const docs = (res?.results || []).map((r: any) => ({
        id: r.documentId,
        score: r.score,
        snippet: r.chunks?.[0]?.content?.slice(0, 500) || "",
      }));

      return docs;
    } catch (e) {
      // graceful fallback
      return [];
    }
  }

  /**
   * Generate a reply with optional quick replies.
   */
  static async reply(input: AgentRuntimeReplyInput): Promise<AgentRuntimeReply> {
    const cfg = (await this.getActiveConfig(input.clientId)) || {
      id: null,
      name: "default",
      tonality: "professional",
      responseStyle: "helpful",
      dosList: [],
      dontsList: [],
      personality: "professional",
      industry: "automotive",
      model: "openai/gpt-4o-mini" as ModelId,
    };

    const system = this.buildSystemPrompt(cfg);

    const memoryDocs = await this.recallMemory({
      clientId: input.clientId,
      leadId: input.leadId,
      topic: input.topic,
    });

    const memoryBlock =
      memoryDocs.length > 0
        ? `\nRelevant memory:\n${memoryDocs
            .map((d) => `- [${d.score.toFixed(2)}] ${d.snippet.replace(/\n+/g, " ")}`)
            .join("\n")}`
        : "";

    const userPrompt = [
      `Lead message: "${input.message}"`,
      input.topic ? `Topic hint: ${input.topic}` : "",
      memoryBlock,
      `\nRespond naturally and helpfully. If appropriate, include one clear call-to-action.`,
      `Return JSON: {"reply": "...", "quickReplies": ["...","..."]}`,
    ]
      .filter(Boolean)
      .join("\n");

    // Generate with JSON enforced
    const res = await LLMClient.generate({
      model: input.model || cfg.model || "openai/gpt-4o-mini",
      system,
      user: userPrompt,
      json: true,
      temperature: 0.2,
      maxTokens: input.maxTokens ?? 700,
    });

    let parsed: { reply?: string; quickReplies?: string[] } = {};
    try {
      parsed = JSON.parse(res.content);
    } catch {
      parsed = { reply: "Thanks for reaching out—could you share a bit more?" };
    }

    // Write the AI reply to memory (best-effort)
    try {
      const tags = [
        `client:${input.clientId}`,
        input.leadId ? `lead:${input.leadId}` : null,
      ].filter(Boolean) as string[];

      await supermemory.memories.add({
        content: `[AI Reply]\n${parsed.reply || ""}`,
        containerTags: tags,
        metadata: {
          type: "ai_reply",
          conversationId: input.conversationId,
        },
      });
    } catch {
      /* ignore */
    }

    return {
      reply: parsed.reply || "Thanks for the message—how can I help?",
      quickReplies: parsed.quickReplies?.slice(0, 4) || [],
      usedConfigId: cfg.id,
      memoryContextDocs: memoryDocs,
    };
  }
}

Notes
	•	supermemory.search/supermemory.memories.add should be your thin wrapper around the Supermemory SDK you already wrote.
	•	If you prefer your QueryBuilder/MemoryMapper, swap those in—runtime only cares that queries are scoped by tags.

⸻

3) Minimal API route

// server/routes/agent.ts
import { Router } from "express";
import { AgentRuntime } from "../services/agent-runtime";
import { requireAuth } from "../middleware/auth"; // your existing auth middleware

export const agentRouter = Router();

/**
 * POST /api/agent/reply
 * Body: { message, leadId?, conversationId?, topic? }
 * Resolves clientId from auth/session.
 */
agentRouter.post("/reply", requireAuth, async (req, res) => {
  try {
    const clientId = req.user.clientId; // ensure your auth middleware sets this
    const { message, leadId, conversationId, topic, model } = req.body;

    if (!message || !clientId) {
      return res.status(400).json({ error: "message and clientId are required" });
    }

    const out = await AgentRuntime.reply({
      clientId,
      message,
      leadId,
      conversationId,
      topic,
      model,
    });

    res.json(out);
  } catch (e: any) {
    res.status(500).json({ error: e.message || "Failed to generate reply" });
  }
});

Register the route:

// server/index-secure.ts (or wherever you wire routes)
import { agentRouter } from "./routes/agent";
app.use("/api/agent", agentRouter);


⸻

4) Frontend usage (very light)

// client/api/agent.ts
export async function generateAgentReply(payload: {
  message: string;
  leadId?: string;
  conversationId?: string;
  topic?: string;
}) {
  const res = await fetch("/api/agent/reply", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload),
  });
  if (!res.ok) throw new Error(`Agent reply failed: ${res.status}`);
  return res.json();
}


⸻

5) Supermemory scoping checklist

When you ingest or search, always include:

containerTags: [
  `client:${clientId}`,
  `lead:${sha256(email)}`  // or leadId if you prefer
]

You’ve already implemented hashing and multi-tenant isolation—this runtime just enforces it from the agent side.

⸻

6) Ops/observability (minimal)
	•	Log clientId, configId, latency, token count from LLMClient.generate.
	•	Add a /api/agent/health that does a no-op call to verify config + Supermemory connectivity if you want a quick probe.

⸻

7) Safety defaults
	•	If no active config exists, runtime uses conservative defaults and still responds.
	•	If Supermemory is down, runtime skips recall gracefully.
	•	JSON formatting is enforced by LLMClient with retries.

⸻

If you want me to wire this directly into your existing conversation send path (so replies flow into the same storage and WS broadcast), say the word and I’ll hand you the small diff for live-conversation/websocket integration.